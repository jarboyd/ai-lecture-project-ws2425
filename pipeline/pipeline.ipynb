{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    " This pipeline should pipe an image through the localization model and the recognition model to first localize a sign in the image and then decide which sign it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "At first, we have to install  and import some libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdlib\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# vs_build_tools heruntergeladen und installiert, dabei habe ich c++ ausgewählt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# dann pip install dlib\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "# vs_build_tools heruntergeladen und installiert, dabei habe ich c++ ausgewählt\n",
    "# dann pip install dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the image\n",
    "\n",
    "To import your own images, edit the path of the example_images_folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_images_folder = \"images\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to work with the data, convert .ppm into .jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#pip install opencv-python\n",
    "import cv2\n",
    "# pip install numpy\n",
    "import numpy as np\n",
    "\n",
    "__current_directory = os.getcwd()\n",
    "dataset_path = os.path.join(__current_directory, example_images_folder)\n",
    "\n",
    "for file in os.listdir(dataset_path):\n",
    "        if not file.endswith(\".ppm\"):\n",
    "            continue\n",
    "        el_image = cv2.imread(os.path.join(dataset_path, file))\n",
    "        cv2.imwrite(os.path.join(dataset_path, file.replace(\".ppm\", \".jpg\")), el_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dlib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m models_directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../localization/resources\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m detectors \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 3\u001b[0m     model[\u001b[38;5;241m6\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]: \u001b[43mdlib\u001b[49m\u001b[38;5;241m.\u001b[39msimple_object_detector(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(models_directory, model))\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(models_directory)\n\u001b[0;32m      5\u001b[0m }\n\u001b[0;32m      8\u001b[0m images_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFullIJCNN2013\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m confidence_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Confidence threshold to display detections\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dlib' is not defined"
     ]
    }
   ],
   "source": [
    "models_directory = os.path.join(os.getcwd(), \"../localization/resources\", \"models\")\n",
    "detectors = {\n",
    "    model[6:-4]: dlib.simple_object_detector(os.path.join(models_directory, model))\n",
    "    for model in os.listdir(models_directory)\n",
    "}\n",
    "\n",
    "\n",
    "images_folder = os.path.join(os.getcwd(), \"FullIJCNN2013\")\n",
    "confidence_threshold = 0.5  # Confidence threshold to display detections\n",
    "\n",
    "# Adjusted text for readability\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 0.7  # Larger scale for better readability\n",
    "font_thickness = 2  # Thicker text for better visibility\n",
    "\n",
    "img = cv2.imread(os.path.join(dataset_path, \"00000.png\"))\n",
    "\n",
    "# Assuming 'boxes', 'confidences', 'detector_idxs' are returned from your model\n",
    "\n",
    "boxes, confidences, detector_idxs = dlib.simple_object_detector.run_multiple(\n",
    "    list(detectors.values()), img, upsample_num_times=1, adjust_threshold=0.0\n",
    ")\n",
    "annotation_position = (img.shape[1] - 250, 30)  # Position near the top-right corner\n",
    "cv2.putText(img, \"custom_annotation\", annotation_position, font, font_scale, (0, 0, 255), font_thickness)\n",
    "\n",
    "for i in range(len(boxes)):\n",
    "    if confidences[i] < confidence_threshold:\n",
    "        continue  # Skip if confidence is below threshold\n",
    "    \n",
    "    box = boxes[i]\n",
    "    label = f\"Detector {list(detectors.keys())[detector_idxs[i]]}: {confidences[i]:.2f}\"\n",
    "\n",
    "    # Draw bounding box\n",
    "    cv2.rectangle(img, (box.left(), box.top()), (box.right(), box.bottom()), (0, 0, 255), 2)\n",
    "\n",
    "    # Display label and confidence below the box\n",
    "    text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "    text_x = box.left()\n",
    "    text_y = box.bottom() + text_size[1] + 5\n",
    "\n",
    "    cv2.putText(img, label, (text_x, text_y), font, font_scale, (0, 0, 255), font_thickness)\n",
    "\n",
    "# Display the image with annotations\n",
    "cv2.imshow(\"Annotated Image\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"./images/00000.jpg\")\n",
    "img = cv2.resize(img, (64, 64))\n",
    "img = np.expand_dims(img, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alisa\\Desktop\\Ki Projekt\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('../recognition/final_model.h5', compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = model.predict(img)\n",
    "predicted_class_id = np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep right\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../recognition/\")\n",
    "from constants import CLASS_ID_TO_NAME\n",
    "\n",
    "predicted_class_name = CLASS_ID_TO_NAME.get(predicted_class_id, \"Unknown Class\")\n",
    "\n",
    "print(predicted_class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
